# Core ML stack (install torch with the correct CUDA 11.8 wheel separately)
transformers>=4.56.2      # 3.13 classifier, modern HF stack:contentReference[oaicite:0]{index=0}
accelerate>=1.12.0        # Python>=3.10, works with current HF ecosystem:contentReference[oaicite:1]{index=1}
datasets>=4.4.1           # has explicit Python 3.13 support:contentReference[oaicite:2]{index=2}
peft>=0.18.0              # classifiers include Python 3.13:contentReference[oaicite:3]{index=3}
trl>=0.12.0               # recent TRL; tested with current Transformers/PEFT
bitsandbytes>=0.48.2      # Python 3.13 + CUDA â‰¥11.8 supported:contentReference[oaicite:4]{index=4}
sentencepiece>=0.2.0
safetensors>=0.7.0        # Python>=3.9; conda builds target py3.13 as well:contentReference[oaicite:5]{index=5}

# Evaluation and utilities
evaluate>=0.4.2
rich>=13.7.1              # known to install cleanly on 3.13 in practice:contentReference[oaicite:6]{index=6}
wandb>=0.18.0
jsonlines>=4.0.0
pyyaml>=6.0.2
pytest>=8.3.2


