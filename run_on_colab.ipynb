{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca1d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r DSAN-5800-LoRA-Project/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f31a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repo (or upload files)\n",
    "!git clone https://github.com/mandyjsun/DSAN-5800-LoRA-Project.git\n",
    "%cd DSAN-5800-LoRA-Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede2775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare data\n",
    "!mkdir -p data/processed artifacts/checkpoints artifacts/metrics\n",
    "!python scripts/prepare_data.py \\\n",
    "  --out-dir data/processed \\\n",
    "  --train-ratio 0.8 --val-ratio 0.1 --test-ratio 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4448c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Baseline inference (zero-shot)\n",
    "!python scripts/baseline_infer.py \\\n",
    "  --model mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    "  --mbpp-file data/processed/mbpp_test.jsonl \\\n",
    "  --out-file artifacts/metrics/baseline_generations.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b6ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train LoRA (rank 8)\n",
    "!python scripts/train_lora.py \\\n",
    "  --config configs/lora_r8.yaml \\\n",
    "  --train-file data/processed/train.jsonl \\\n",
    "  --val-file data/processed/val.jsonl \\\n",
    "  --output-dir artifacts/checkpoints/mistral7b-code-r8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe82a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluate LoRA r=8 on MBPP\n",
    "!python scripts/eval_mbpp.py \\\n",
    "  --base-model mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    "  --adapter-path artifacts/checkpoints/mistral7b-code-r8 \\\n",
    "  --mbpp-file data/processed/mbpp_test.jsonl \\\n",
    "  --out-dir artifacts/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d24055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train LoRA (rank 32) - OPTIONAL (uses more VRAM)\n",
    "!python scripts/train_lora.py \\\n",
    "  --config configs/lora_r32.yaml \\\n",
    "  --train-file data/processed/train.jsonl \\\n",
    "  --val-file data/processed/val.jsonl \\\n",
    "  --output-dir artifacts/checkpoints/mistral7b-code-r32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluate LoRA r=32 on MBPP\n",
    "!python scripts/eval_mbpp.py \\\n",
    "  --base-model mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    "  --adapter-path artifacts/checkpoints/mistral7b-code-r32 \\\n",
    "  --mbpp-file data/processed/mbpp_test.jsonl \\\n",
    "  --out-dir artifacts/metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0657cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "from google.colab import files\n",
    "!zip -r artifacts.zip artifacts/\n",
    "files.download('artifacts.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
